{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages from config/init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing AWS-cli configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using access_key: ........ZYMLD6V5WV\n"
     ]
    }
   ],
   "source": [
    "access_key = !aws configure get aws_access_key_id\n",
    "secret_access_key = !aws configure get aws_secret_access_key\n",
    "if access_key and secret_access_key:\n",
    "    print('Using access_key: ........{}'.format(access_key[0][10:]))\n",
    "else:\n",
    "    print('Please, configure AWS-cli before running this notebook')\n",
    "    print('Open a Terminal and run: aws configure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "Edit AWS zone and region variable accordingly to your geographical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east-1'\n",
    "ZONE = 'us-east-1c'\n",
    "!aws configure set region {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining global Tags for identifying resources\n",
    "\n",
    "Associate AWS Tags with each resource created by this notebook helps to compile total cost used by AWS.\n",
    "The notebook will use, if exists, a file in the **CONFIG/aws** folder named: **aws-tags.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project tag: cbb-research-dl\n"
     ]
    }
   ],
   "source": [
    "TAGFILE_S3 = None\n",
    "TAGFILE = None\n",
    "TAGDIR = None\n",
    "if os.path.exists(os.path.join(CONFIG, \"aws\", \"aws-tags-s3.json\")):\n",
    "    TAGFILE_S3 = os.path.join(CONFIG, \"aws\", \"aws-tags-s3.json\")\n",
    "if os.path.exists(os.path.join(CONFIG, \"aws\", \"aws-tags.json\")):\n",
    "    TAGFILE = os.path.join(CONFIG, \"aws\", \"aws-tags.json\")\n",
    "    with open(TAGFILE) as fin:\n",
    "        TAGDICT = json.loads(fin.read())\n",
    "    PROJECT = None\n",
    "    for k in TAGDICT['Tags']:\n",
    "        if k['Key'] == 'Project':\n",
    "            PROJECT = k['Value']\n",
    "    if PROJECT:\n",
    "        print(\"Using project tag: {}\".format(PROJECT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS machine types\n",
    "\n",
    "| Instance Size | vCPU | Memory (GiB) | Instance Storage (GiB) | Network Bandwidth (Gbps) | EBS Bandwidth (Mbps) | $/Hour |\n",
    "|---------|----------|----------|-------------|---------------|---------------|-----------|\n",
    "| m5d.4xlarge | 16 | 64 | 2 x 300 NVMe SSD | Up to 10 | 4,750 | 0.904 |\n",
    "| m5d.8xlarge | 32 | 128 | 2 x 600 NVMe SSD | 10 | 6,800 | 1.808 |\n",
    "| m5d.16xlarge | 64 | 256 | 4 x 600 NVMe SSD | 20 | 13,600 | 3.616 |\n",
    "| m5dn.4xlarge | 16 | 64 | 2 x 300 NVMe SSD | Up to 25 | 4,750 | 1.088 |\n",
    "| m5dn.8xlarge | 32 | 128 | 2 x 600 NVMe SSD | 25 | 6,800| 2.176 |\n",
    "| m5dn.16xlarge | 64 | 256 | 4 x 600 NVMe SSD | 75 | 13,600 | 4.352 |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_SIZES = [2000, 6000, 10000]\n",
    "\n",
    "MACHINE_TYPES = ['m5d', 'm5dn']\n",
    "CPUs = [16, 32, 64]\n",
    "\n",
    "# Prices from 03/04/2020\n",
    "PRICE = {\n",
    "    'n1':{\n",
    "        16: 0.904,\n",
    "        32: 1.808,\n",
    "        64: 3.616\n",
    "    },\n",
    "    'n2':{\n",
    "        16: 1.088,\n",
    "        32: 2.176,\n",
    "        64: 4.352\n",
    "    }    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using as output directory: /panfs/pan1.be-md.ncbi.nlm.nih.gov/alt_splicing/cloud-transcriptome-annotation/results/PRJNA320545\n"
     ]
    }
   ],
   "source": [
    "result_dir = os.path.join(RESULTS, DATASET)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir) \n",
    "os.chdir(result_dir)\n",
    "print('Using as output directory: {}'.format(result_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or retrieve AWS S3 storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a\n",
      "make_bucket: nopal-results-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a\n",
      "upload: 2000/fasta/2000_12.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_12.fa\n",
      "upload: 2000/fasta/2000_1.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_1.fa\n",
      "upload: 2000/fasta/2000_11.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_11.fa\n",
      "upload: 2000/fasta/2000_15.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_15.fa\n",
      "upload: 2000/fasta/2000_13.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_13.fa\n",
      "upload: 2000/fasta/2000_16.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_16.fa\n",
      "upload: 2000/fasta/2000_14.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_14.fa\n",
      "upload: 2000/fasta/2000_10.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_10.fa\n",
      "upload: 2000/fasta/2000_17.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_17.fa\n",
      "upload: 2000/fasta/2000_18.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_18.fa\n",
      "upload: 2000/fasta/2000_19.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_19.fa\n",
      "upload: 2000/fasta/2000_2.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_2.fa\n",
      "upload: 2000/fasta/2000_5.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_5.fa\n",
      "upload: 2000/fasta/2000_3.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_3.fa\n",
      "upload: 2000/fasta/2000_4.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_4.fa\n",
      "upload: 2000/fasta/2000_6.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_6.fa\n",
      "upload: 2000/fasta/2000_8.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_8.fa\n",
      "upload: 2000/fasta/2000_7.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_7.fa\n",
      "upload: 2000/fasta/2000_20.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_20.fa\n",
      "upload: 2000/fasta/2000_9.fa to s3://nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a/2000_9.fa\n",
      "make_bucket: nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9\n",
      "make_bucket: nopal-results-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9\n",
      "upload: 6000/fasta/6000_13.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_13.fa\n",
      "upload: 6000/fasta/6000_15.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_15.fa\n",
      "upload: 6000/fasta/6000_16.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_16.fa\n",
      "upload: 6000/fasta/6000_12.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_12.fa\n",
      "upload: 6000/fasta/6000_17.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_17.fa\n",
      "upload: 6000/fasta/6000_1.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_1.fa\n",
      "upload: 6000/fasta/6000_18.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_18.fa\n",
      "upload: 6000/fasta/6000_11.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_11.fa\n",
      "upload: 6000/fasta/6000_10.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_10.fa\n",
      "upload: 6000/fasta/6000_14.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_14.fa\n",
      "upload: 6000/fasta/6000_19.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_19.fa\n",
      "upload: 6000/fasta/6000_6.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_6.fa\n",
      "upload: 6000/fasta/6000_20.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_20.fa\n",
      "upload: 6000/fasta/6000_4.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_4.fa\n",
      "upload: 6000/fasta/6000_7.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_7.fa\n",
      "upload: 6000/fasta/6000_8.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_8.fa\n",
      "upload: 6000/fasta/6000_9.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_9.fa\n",
      "upload: 6000/fasta/6000_2.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_2.fa\n",
      "upload: 6000/fasta/6000_5.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_5.fa\n",
      "upload: 6000/fasta/6000_3.fa to s3://nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9/6000_3.fa\n",
      "make_bucket: nopal-10000-7328e20f-5713-465a-b394-5739a9d08698\n",
      "make_bucket: nopal-results-10000-7328e20f-5713-465a-b394-5739a9d08698\n",
      "upload: 10000/fasta/10000_18.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_18.fa\n",
      "upload: 10000/fasta/10000_12.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_12.fa\n",
      "upload: 10000/fasta/10000_16.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_16.fa\n",
      "upload: 10000/fasta/10000_13.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_13.fa\n",
      "upload: 10000/fasta/10000_15.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_15.fa\n",
      "upload: 10000/fasta/10000_11.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_11.fa\n",
      "upload: 10000/fasta/10000_19.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_19.fa\n",
      "upload: 10000/fasta/10000_1.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_1.fa\n",
      "upload: 10000/fasta/10000_14.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_14.fa\n",
      "upload: 10000/fasta/10000_17.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_17.fa\n",
      "upload: 10000/fasta/10000_10.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_10.fa\n",
      "upload: 10000/fasta/10000_20.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_20.fa\n",
      "upload: 10000/fasta/10000_2.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_2.fa\n",
      "upload: 10000/fasta/10000_5.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_5.fa\n",
      "upload: 10000/fasta/10000_4.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_4.fa\n",
      "upload: 10000/fasta/10000_6.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_6.fa\n",
      "upload: 10000/fasta/10000_3.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_3.fa\n",
      "upload: 10000/fasta/10000_7.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_7.fa\n",
      "upload: 10000/fasta/10000_9.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_9.fa\n",
      "upload: 10000/fasta/10000_8.fa to s3://nopal-10000-7328e20f-5713-465a-b394-5739a9d08698/10000_8.fa\n",
      "Query size: 2000\n",
      "\tin-bucket: nopal_2000_97f602dc-d34a-4f98-8d37-492abfb0d83a\n",
      "\tout-bucket: nopal_results2000_97f602dc-d34a-4f98-8d37-492abfb0d83a\n",
      "Query size: 6000\n",
      "\tin-bucket: nopal_6000_1c8b7e48-482e-4a7f-a02c-0710949d5fa9\n",
      "\tout-bucket: nopal_results6000_1c8b7e48-482e-4a7f-a02c-0710949d5fa9\n",
      "Query size: 10000\n",
      "\tin-bucket: nopal_10000_7328e20f-5713-465a-b394-5739a9d08698\n",
      "\tout-bucket: nopal_results10000_7328e20f-5713-465a-b394-5739a9d08698\n"
     ]
    }
   ],
   "source": [
    "bucket_list = !aws s3 ls | awk '{print $3}'\n",
    "buckets = {}\n",
    "for q in QUERY_SIZES:\n",
    "    prefix = 'nopal-' + str(q) + '-'\n",
    "    suffix = None\n",
    "    for l in bucket_list:\n",
    "        if prefix in l:\n",
    "            suffix = l.replace('nopal-' + str(q) + '-','')\n",
    "            break\n",
    "    if suffix:\n",
    "        buckets[q] = suffix    \n",
    "\n",
    "for q in QUERY_SIZES:\n",
    "    if q not in buckets:\n",
    "        suffix = str(uuid.uuid4())\n",
    "        inbucket = 'nopal-' + str(q) + '-' + suffix\n",
    "        outbucket = 'nopal-results-' + str(q) + '-' + suffix\n",
    "        buckets[q] = suffix \n",
    "        \n",
    "        !aws s3 mb s3://{inbucket} --region {REGION}          \n",
    "        !aws s3 mb s3://{outbucket} --region {REGION}\n",
    "        if TAGFILE_S3:\n",
    "            !aws s3api put-bucket-tagging --bucket {inbucket} --tagging file://{TAGFILE_S3} \n",
    "            !aws s3api put-bucket-tagging --bucket {outbucket} --tagging file://{TAGFILE_S3} \n",
    "        !aws s3 cp {q}/fasta/ s3://{inbucket}/ --recursive\n",
    "        \n",
    "for q in buckets:\n",
    "    print('Query size: {0}\\n\\tin-bucket: nopal-{0}-{1}\\n\\tout-bucket: nopal-results{0}-{1}'.format(q, buckets[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a AWS Batch unmanaged Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Internet gateway.\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/reference/ec2/create-internet-gateway.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Internet Gateway: igw-0941f7aeb58334343\n"
     ]
    }
   ],
   "source": [
    "igw = !aws ec2 describe-internet-gateways --filters Name=tag:Project,Values={PROJECT}\n",
    "igw = json.loads(''.join(igw))   \n",
    "if 'InternetGateways' in igw and len(igw['InternetGateways']) > 0:\n",
    "    igw = igw['InternetGateways'][0]\n",
    "    print('Using Internet Gateway: {}'.format(igw['InternetGatewayId']))\n",
    "else:\n",
    "    igw = !aws ec2 create-internet-gateway\n",
    "    igw = json.loads(''.join(igw))\n",
    "    if 'InternetGateway' in igw and 'InternetGatewayId' in igw['InternetGateway']:\n",
    "        igw = igw['InternetGateway']\n",
    "        print('Created Internet Gateway: {}'.format(igw['InternetGatewayId']))\n",
    "        if TAGFILE:\n",
    "            igw_id = igw['InternetGatewayId']\n",
    "            !aws ec2 create-tags --resources {igw_id} --cli-input-json file://{TAGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Amazon Virtual Private Cloud  (VPC) and all its componets\n",
    "\n",
    "* VPC: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-vpc.html\n",
    "* ACL: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-network-acl.html\n",
    "* Route Table: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-route-table.html\n",
    "* Subnet: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-subnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VPC: vpc-0811acb4a1a497102\n",
      "Subnet subnet-076bdd7bddb7c51cc attached to VPC vpc-0811acb4a1a497102\n"
     ]
    }
   ],
   "source": [
    "vpc = !aws ec2 describe-vpcs --filters Name=tag:Project,Values={PROJECT}\n",
    "vpc = json.loads(''.join(vpc))\n",
    "if 'Vpcs' in vpc and len(vpc['Vpcs']) > 0:\n",
    "    vpc = vpc['Vpcs'][0]\n",
    "    vpc_id = vpc['VpcId']\n",
    "    print('Using VPC: {}'.format(vpc['VpcId']))\n",
    "    subnet = !aws ec2 describe-subnets --filters \"Name=vpc-id,Values={vpc_id}\"\n",
    "    subnet = json.loads(''.join(subnet))\n",
    "    if 'Subnets' in subnet:\n",
    "        subnet = subnet['Subnets'][0]   \n",
    "        subnet_id = subnet['SubnetId']\n",
    "\n",
    "        print('Subnet {} attached to VPC {}'.format(subnet_id, vpc_id))    \n",
    "else:\n",
    "    print('No VPC, creating it ..... ')\n",
    "    vpc = !aws ec2 create-vpc --cidr-block 10.0.0.0/16 --amazon-provided-ipv6-cidr-block \n",
    "    vpc = json.loads(''.join(vpc))\n",
    "    if 'Vpc' in vpc:\n",
    "        vpc = vpc['Vpc']\n",
    "        vpc_id = vpc['VpcId']\n",
    "        print('Created VPC: {}'.format(vpc_id))\n",
    "        # adding Tags if file exists\n",
    "        if TAGFILE:        \n",
    "            !aws ec2 create-tags --resources {vpc_id} --cli-input-json file://{TAGFILE}\n",
    "\n",
    "        # Attaching igw\n",
    "        igw_id = igw['InternetGatewayId']\n",
    "        print('Attaching IGW {} to the VPC: {}'.format(igw_id, vpc_id))\n",
    "        !aws ec2 attach-internet-gateway --internet-gateway-id {igw_id} --vpc-id {vpc_id}\n",
    "\n",
    "        # Retrieving created ACL\n",
    "        acl = !aws ec2 describe-network-acls --filters Name=vpc-id,Values={vpc_id}\n",
    "        acl = json.loads(''.join(acl))\n",
    "        if 'NetworkAcls' in acl and len(acl['NetworkAcls']) == 1:\n",
    "            acl = acl['NetworkAcls'][0]\n",
    "            # adding Tags if file exists\n",
    "            if TAGFILE:                \n",
    "                acl_id = acl['NetworkAclId']\n",
    "                print('Tagging ACL {}'.format(acl_id))\n",
    "                !aws ec2 create-tags --resources {acl_id} --cli-input-json file://{TAGFILE}\n",
    "\n",
    "        # Retrieving created routes\n",
    "        route = !aws ec2 describe-route-tables --filters Name=vpc-id,Values={vpc_id}\n",
    "        route = json.loads(''.join(route))\n",
    "        if 'RouteTables' in route and len(route['RouteTables']) == 1:\n",
    "            route = route['RouteTables'][0]\n",
    "            route_id = route['RouteTableId']\n",
    "                \n",
    "            route_igw = !aws ec2 create-route --route-table-id {route_id} --destination-cidr-block 0.0.0.0/0 --gateway-id {igw_id}\n",
    "            route_igw = json.loads(''.join(route_igw))\n",
    "            if 'Return' in route_igw and route_igw['Return']:\n",
    "                print('IGW {} attached to route {}'.format(igw_id, route_id))\n",
    "            \n",
    "            # adding Tags if file exists\n",
    "            if TAGFILE:\n",
    "                print('Tagging Route {}'.format(route_id))\n",
    "                !aws ec2 create-tags --resources {route_id} --cli-input-json file://{TAGFILE}\n",
    "\n",
    "        # Creating Subnets\n",
    "        subnet = !aws ec2 create-subnet --vpc-id {vpc_id} --cidr-block 10.0.0.0/16 --availability-zone {ZONE}\n",
    "        subnet = json.loads(''.join(subnet))\n",
    "        if 'Subnet' in subnet:\n",
    "            subnet = subnet['Subnet']   \n",
    "            subnet_id = subnet['SubnetId']\n",
    "            \n",
    "            print('Subnet {} attached to VPC {}'.format(subnet_id, vpc_id))\n",
    "            !aws ec2 modify-subnet-attribute --subnet-id {subnet_id} --map-public-ip-on-launch\n",
    "            print('Public IPs enable on subnet {}'.format(subnet_id))\n",
    "            \n",
    "            # adding Tags if file exists\n",
    "            if TAGFILE:\n",
    "                print('Tagging subnet {}'.format(subnet_id))\n",
    "                !aws ec2 create-tags --resources {subnet_id} --cli-input-json file://{TAGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the ARN for AWSBatchServiceRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWSBatchServiceRole Arn: arn:aws:iam::250813660784:role/service-role/AWSBatchServiceRole\n",
      "Project Role does not exists.\n",
      "Creating the role\n",
      "ERROR: ['', 'An error occurred (AccessDenied) when calling the CreateRole operation: User: arn:aws:iam::250813660784:user/veraalva is not authorized to perform: iam:CreateRole on resource: arn:aws:iam::250813660784:role/cbb-research-dl-batch-role']\n",
      "Using s3_role Arn: arn:aws:iam::250813660784:role/cbb-research-db-batch-role\n"
     ]
    }
   ],
   "source": [
    "batchRole = !aws iam get-role --role-name AWSBatchServiceRole\n",
    "try:\n",
    "    batchRole = json.loads(''.join(batchRole))\n",
    "    if 'Role' in batchRole:\n",
    "        batchRole = batchRole['Role']\n",
    "        batchRole_arn = batchRole['Arn']\n",
    "        print('Using AWSBatchServiceRole Arn: {}'.format(batchRole_arn))\n",
    "    \n",
    "    batchRole = !aws iam get-role --role-name AWSBatchServiceRole\n",
    "except:\n",
    "    print('AWSBatchServiceRole does not exists.')\n",
    "    print('Please create the role as described here: https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html')\n",
    "    print('Then, run this cell again')\n",
    "    print('ERROR: {}'.format(batchRole))\n",
    "\n",
    "s3_rolefile = os.path.join(CONFIG, 'aws', 's3-role.json') \n",
    "s3_role = None\n",
    "with open(s3_rolefile) as fin:\n",
    "    s3_role = json.loads(fin.read())\n",
    "    s3_role['Statement'][0]['Resource'] = []\n",
    "    for q in buckets:\n",
    "        s3_role['Statement'][0]['Resource'].append('arn:aws:s3:::nopal-{0}-{1}'.format(q,buckets[q]))\n",
    "        s3_role['Statement'][0]['Resource'].append('arn:aws:s3:::nopal-results-{0}-{1}'.format(q,buckets[q]))\n",
    "if s3_role:\n",
    "    with open(s3_rolefile, 'w') as fout:\n",
    "        fout.write(json.dumps(s3_role, indent=4) + '\\n')\n",
    "    \n",
    "output = !aws iam get-role --role-name {PROJECT}-batch-role  \n",
    "try:\n",
    "    s3_role = json.loads(''.join(output))\n",
    "    if 'Role' in s3_role:\n",
    "        s3_role = s3_role['Role']\n",
    "        s3_role_arn = s3_role['Arn']\n",
    "        print('Using s3_role Arn: {}'.format(s3_role_arn))\n",
    "    else:\n",
    "        output = !aws iam create-role --role-name {PROJECT}-batch-role --assume-role-policy-document file://{s3_rolefile}\n",
    "        s3_role = json.loads(''.join(output))\n",
    "except:\n",
    "    print('Project Role does not exists.')\n",
    "    print('Creating the role')\n",
    "    try:\n",
    "        output = !aws iam create-role --role-name {PROJECT}-batch-role --assume-role-policy-document file://{s3_rolefile}\n",
    "        s3_role = json.loads(''.join(output))\n",
    "    except:\n",
    "        print('ERROR: {}'.format(output))\n",
    "\n",
    "# Delete this after testing the role creation         \n",
    "output = !aws iam get-role --role-name cbb-research-db-batch-role\n",
    "try:\n",
    "    s3_role = json.loads(''.join(output))\n",
    "    if 'Role' in s3_role:\n",
    "        s3_role = s3_role['Role']\n",
    "        s3_role_arn = s3_role['Arn']\n",
    "        print('Using s3_role Arn: {}'.format(s3_role_arn))    \n",
    "except:\n",
    "    print('Project Role does not exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating AWS Batch Components\n",
    "\n",
    " * Computational environment: https://docs.aws.amazon.com/cli/latest/reference/batch/create-compute-environment.html\n",
    " * Batch queue: https://docs.aws.amazon.com/cli/latest/reference/batch/create-job-queue.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmanaged compute environment does not exist. Creating it ....\n",
      "Compute environment: arn:aws:batch:us-east-1:250813660784:compute-environment/cbb-research-dl-unmanaged\n",
      "Queue does not exist. Creating it ....\n",
      "Queue arn:aws:batch:us-east-1:250813660784:job-queue/cbb-research-dl-queue\n"
     ]
    }
   ],
   "source": [
    "comp_env = None\n",
    "queue = None\n",
    "output = !aws batch describe-compute-environments\n",
    "try:\n",
    "    comp_envs = json.loads(''.join(output))    \n",
    "    if 'computeEnvironments' in comp_envs:\n",
    "        for c in comp_envs['computeEnvironments']:\n",
    "            if c['computeEnvironmentName'] == '{}-unmanaged'.format(PROJECT):\n",
    "                comp_env = c\n",
    "                break\n",
    "    if not comp_env:\n",
    "        print('Unmanaged compute environment does not exist. Creating it ....')\n",
    "        output = !aws batch create-compute-environment --compute-environment-name {PROJECT}-unmanaged --type UNMANAGED --state ENABLED --service-role {batchRole_arn}\n",
    "        comp_env = json.loads(''.join(output))\n",
    "    \n",
    "    if comp_env:\n",
    "        print('Compute environment: {}'.format(comp_env['computeEnvironmentArn']))\n",
    "        output = !aws batch describe-job-queues\n",
    "        queues = json.loads(''.join(output))  \n",
    "        queue = None\n",
    "        if 'jobQueues' in queues:\n",
    "            for c in queues['jobQueues']:\n",
    "                if c['jobQueueName'] == '{}-queue'.format(PROJECT):\n",
    "                    queue = c\n",
    "                    break\n",
    "        if queue:\n",
    "            print('Queue: {}'.format(queue['jobQueueArn']))\n",
    "        else:\n",
    "            print('Queue does not exist. Creating it ....') \n",
    "            compu_env_arn = comp_env['computeEnvironmentArn']\n",
    "            output = !aws batch create-job-queue --job-queue-name {PROJECT}-queue --state ENABLED --priority 1 --compute-environment-order order=1,computeEnvironment={compu_env_arn}\n",
    "            queue = json.loads(''.join(output))    \n",
    "            print('Queue {}'.format(queue['jobQueueArn']))\n",
    "        if not queue:\n",
    "            print('ERROR: No AWS Batch queue available. Please, check possible errors')    \n",
    "except ex:\n",
    "    print('ERROR: {}'.format(output))  \n",
    "    print(e)\n",
    "\n",
    "if not comp_env:\n",
    "    print('ERROR: No computational environment available. Please, check possible errors')\n",
    "if not queue:\n",
    "    print('ERROR: No AWS Batch queue available. Please, check possible errors')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ECS cluster name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECS cluster: arn:aws:ecs:us-east-1:250813660784:cluster/cbb-research-dl-unmanaged_Batch_ad56ff41-45e2-36e9-855f-7f87e2251d68\n"
     ]
    }
   ],
   "source": [
    "ecs_cluster = None\n",
    "output = !aws batch describe-compute-environments\n",
    "try:\n",
    "    comp_envs = json.loads(''.join(output))    \n",
    "    if 'computeEnvironments' in comp_envs:\n",
    "        for c in comp_envs['computeEnvironments']:\n",
    "            if c['computeEnvironmentName'] == '{}-unmanaged'.format(PROJECT):\n",
    "                comp_env = c\n",
    "                ecs_cluster = c['ecsClusterArn']\n",
    "                break\n",
    "except ex:\n",
    "    print('ERROR: {}'.format(output))  \n",
    "    print(e)\n",
    "\n",
    "if not ecs_cluster:\n",
    "    print('ERROR: No ECS cluster available. Please, check possible errors')\n",
    "print('ECS cluster: {}'.format(ecs_cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating EC2 components\n",
    "\n",
    " * Security Group: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-security-group.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sg sg-05c7b9ccd93afc9c1\n"
     ]
    }
   ],
   "source": [
    "sg = !aws ec2 describe-security-groups --filters Name=tag:Project,Values={PROJECT}\n",
    "sg = json.loads(''.join(sg))\n",
    "if 'SecurityGroups' in sg and len(sg['SecurityGroups']) >= 1:\n",
    "    sg = sg['SecurityGroups'][0]\n",
    "    sg_id = sg['GroupId']\n",
    "    print('Using sg {}'.format(sg_id))\n",
    "else:\n",
    "    sg_name = PROJECT\n",
    "    sg_descr = 'Security Group for project: ' + PROJECT\n",
    "    sg = !aws ec2 create-security-group --group-name {sg_name}  --description \"{sg_descr}\" --vpc-id {vpc_id}\n",
    "    sg = json.loads(''.join(sg))\n",
    "    if 'GroupId' in sg:\n",
    "        sg_id = sg['GroupId']\n",
    "        print('SG {} created'.format(sg_id))\n",
    "        \n",
    "        print('Adding SSH inbound to the sg {}'.format(sg_id))\n",
    "        !aws ec2 authorize-security-group-ingress --group-id {sg_id} --protocol tcp --port 22 --cidr 0.0.0.0/0\n",
    "        \n",
    "        # adding Tags if file exists\n",
    "        if TAGFILE:\n",
    "            print('Tagging sg {}'.format(sg_id))\n",
    "            !aws ec2 create-tags --resources {sg_id} --cli-input-json file://{TAGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ECS cluster init script for the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = 'echo ECS_CLUSTER={} >> /etc/ecs/ecs.config\\nsystemctl enable --now --no-block ecs.service\\n'.format(comp_env['ecsClusterArn'])\n",
    "user_datafile = os.path.join(CONFIG, 'aws', 'user-data.txt')\n",
    "user_datafile_ecs = os.path.join(CONFIG, 'aws', 'user-data-ecs.txt')\n",
    "with open(user_datafile_ecs) as fin:\n",
    "    with open(user_datafile, 'w') as fout:\n",
    "        fout.write(fin.read() + '\\n')\n",
    "        fout.write(user_data + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching 20 instances for the cluster\n",
    "\n",
    "Using default default Amazon Linux 2 AMI (HVM), SSD Volume Type (ami-0fc61db8544a617ed).\n",
    "In the user data script the following services are installed and configure:\n",
    "\n",
    " * Amazon ECs client\n",
    " * The /dev/nvme1n1 is partitioned, formated (XFS) and mounted in /data\n",
    " * User ec2-user is added to the docker group\n",
    " * The ecs services is configured and started\n",
    " \n",
    "For accessing to the instance using SSH require a SSH key created in the EC2 console (see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of instances to launch\n",
    "NO_INSTANCES = 1\n",
    "\n",
    "# Instance type\n",
    "INSTANCE_TYPE = 'm5d.16xlarge'\n",
    "\n",
    "# If SSH key created you can added it to the instance for login\n",
    "SSH_KEY = \"cbb-research-dl-pk\"\n",
    "\n",
    "# Amazon image\n",
    "IMAGE = \"ami-0fc61db8544a617ed\"\n",
    "\n",
    "cmd = 'aws ec2 run-instances --image-id {} --count {} --instance-type {} '.format(IMAGE, NO_INSTANCES, INSTANCE_TYPE)\n",
    "cmd += '--placement \"AvailabilityZone={}\" '.format(ZONE)\n",
    "cmd += '--key-name {} '.format(SSH_KEY)\n",
    "cmd += '--iam-instance-profile Name=ecsInstanceRole '\n",
    "cmd += '--security-group-ids {} --subnet-id {} '.format(sg_id, subnet_id)\n",
    "cmd += '--user-data file://{} '.format(user_datafile)\n",
    "cmd += '--tag-specifications '\n",
    "\n",
    "#Using defined tags in the instances\n",
    "if TAGDICT:\n",
    "    tags = ''\n",
    "    for t in TAGDICT['Tags']:\n",
    "        if tags:\n",
    "            tags += ','\n",
    "        tags += '{' + 'Key={0},Value={1}'.format(t['Key'], t['Value']) + '}'\n",
    "    cmd += '\\'ResourceType=instance,Tags=[{}]\\' '.format(tags)\n",
    "    cmd += '\\'ResourceType=volume,Tags=[{}]\\''.format(tags)\n",
    "\n",
    "output = !{cmd}\n",
    "try:\n",
    "    instances = json.loads(''.join(output))    \n",
    "except:\n",
    "    print('ERROR: {}'.format(output))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a AWS Batch Job defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definitionfile = os.path.join(CONFIG, 'aws', 'transannot-job-definition.json') \n",
    "job_definition = None\n",
    "with open(job_definitionfile) as fin:\n",
    "    job_definition = json.loads(fin.read())\n",
    "    job_definition['jobDefinitionName'] = '{}-transannotation-job-definition'.format(PROJECT)\n",
    "    job_definition['containerProperties']['jobRoleArn'] = s3_role['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobDefinitionName': 'cbb-research-dl-transannotation-job-definition',\n",
       " 'type': 'container',\n",
       " 'parameters': {},\n",
       " 'containerProperties': {'image': 'gcr.io/cbb-research-dl/transannot',\n",
       "  'vcpus': 64,\n",
       "  'memory': 252000,\n",
       "  'command': ['/usr/envs/transannot/bin/aws-pipeline.sh'],\n",
       "  'jobRoleArn': 'arn:aws:iam::250813660784:role/cbb-research-db-batch-role',\n",
       "  'volumes': [{'host': {'sourcePath': '/data'}, 'name': 'data'}],\n",
       "  'environment': [{'name': 'BLAST_S3_BUCKET_SAMPLE',\n",
       "    'value': 'cbb-research-dl-blastdb'}],\n",
       "  'mountPoints': [{'containerPath': '/data', 'sourceVolume': 'data'}],\n",
       "  'privileged': True,\n",
       "  'ulimits': [],\n",
       "  'user': 'root'}}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
