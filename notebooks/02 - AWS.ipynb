{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages from config/init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing AWS-cli configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using access_key: ........ZYMLD6V5WV\n"
     ]
    }
   ],
   "source": [
    "access_key = !aws configure get aws_access_key_id\n",
    "secret_access_key = !aws configure get aws_secret_access_key\n",
    "if access_key and secret_access_key:\n",
    "    print('Using access_key: ........{}'.format(access_key[0][10:]))\n",
    "else:\n",
    "    print('Please, configure AWS-cli before running this notebook')\n",
    "    print('Open a Terminal and run: aws configure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "Edit AWS zone and region variable accordingly to your geographical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east-1'\n",
    "ZONE = 'us-east-1c'\n",
    "!aws configure set region {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining global Tags for identifying resources\n",
    "\n",
    "Associate AWS Tags with each resource created by this notebook helps to compile total cost used by AWS.\n",
    "The notebook will use, if exists, a file in the **CONFIG/aws** folder named: **aws-tags.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project tag: cbb-research-dl\n"
     ]
    }
   ],
   "source": [
    "TAGFILE_S3 = None\n",
    "TAGFILE = None\n",
    "TAGDIR = None\n",
    "if os.path.exists(os.path.join(CONFIG, \"aws\", \"aws-tags-s3.json\")):\n",
    "    TAGFILE_S3 = os.path.join(CONFIG, \"aws\", \"aws-tags-s3.json\")\n",
    "if os.path.exists(os.path.join(CONFIG, \"aws\", \"aws-tags.json\")):\n",
    "    TAGFILE = os.path.join(CONFIG, \"aws\", \"aws-tags.json\")\n",
    "    with open(TAGFILE) as fin:\n",
    "        TAGDICT = json.loads(fin.read())\n",
    "    PROJECT = None\n",
    "    for k in TAGDICT['Tags']:\n",
    "        if k['Key'] == 'Project':\n",
    "            PROJECT = k['Value']\n",
    "    if PROJECT:\n",
    "        print(\"Using project tag: {}\".format(PROJECT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS machine types\n",
    "\n",
    "| Instance Size | vCPU | Memory (GiB) | Instance Storage (GiB) | Network Bandwidth (Gbps) | EBS Bandwidth (Mbps) | $/Hour |\n",
    "|---------|----------|----------|-------------|---------------|---------------|-----------|\n",
    "| m5d.4xlarge | 16 | 64 | 2 x 300 NVMe SSD | Up to 10 | 4,750 | 0.904 |\n",
    "| m5d.8xlarge | 32 | 128 | 2 x 600 NVMe SSD | 10 | 6,800 | 1.808 |\n",
    "| m5d.16xlarge | 64 | 256 | 4 x 600 NVMe SSD | 20 | 13,600 | 3.616 |\n",
    "| m5dn.4xlarge | 16 | 64 | 2 x 300 NVMe SSD | Up to 25 | 4,750 | 1.088 |\n",
    "| m5dn.8xlarge | 32 | 128 | 2 x 600 NVMe SSD | 25 | 6,800| 2.176 |\n",
    "| m5dn.16xlarge | 64 | 256 | 4 x 600 NVMe SSD | 75 | 13,600 | 4.352 |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_SIZES = [2000, 6000, 10000]\n",
    "\n",
    "MACHINE_TYPES = ['m5d', 'm5dn']\n",
    "CPUs = [\n",
    "    {\n",
    "        'name':'4xlarge',\n",
    "        'CPUs': 16\n",
    "    }, \n",
    "    {\n",
    "        'name':'8xlarge',\n",
    "        'CPUs': 32\n",
    "    }, \n",
    "    {\n",
    "        'name':'16xlarge',\n",
    "        'CPUs': 64\n",
    "    }]\n",
    "\n",
    "# Prices from 03/04/2020\n",
    "PRICE = {\n",
    "    'm5d':{\n",
    "        '4xlarge': 0.904,\n",
    "        '8xlarge': 1.808,\n",
    "        '16xlarge': 3.616\n",
    "    },\n",
    "    'm5dn':{\n",
    "        '4xlarge': 1.088,\n",
    "        '8xlarge': 2.176,\n",
    "        '16xlarge': 4.352\n",
    "    }    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using as output directory: /panfs/pan1.be-md.ncbi.nlm.nih.gov/alt_splicing/cloud-transcriptome-annotation/results/PRJNA320545\n"
     ]
    }
   ],
   "source": [
    "result_dir = os.path.join(RESULTS, DATASET)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir) \n",
    "os.chdir(result_dir)\n",
    "print('Using as output directory: {}'.format(result_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or retrieve AWS S3 storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query size: 2000\n",
      "\tin-bucket: nopal-2000-97f602dc-d34a-4f98-8d37-492abfb0d83a\n",
      "\tout-bucket: nopal-results2000-97f602dc-d34a-4f98-8d37-492abfb0d83a\n",
      "Query size: 6000\n",
      "\tin-bucket: nopal-6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9\n",
      "\tout-bucket: nopal-results6000-1c8b7e48-482e-4a7f-a02c-0710949d5fa9\n",
      "Query size: 10000\n",
      "\tin-bucket: nopal-10000-7328e20f-5713-465a-b394-5739a9d08698\n",
      "\tout-bucket: nopal-results10000-7328e20f-5713-465a-b394-5739a9d08698\n"
     ]
    }
   ],
   "source": [
    "bucket_list = !aws s3 ls | awk '{print $3}'\n",
    "buckets = {}\n",
    "for q in QUERY_SIZES:\n",
    "    prefix = 'nopal-' + str(q) + '-'\n",
    "    suffix = None\n",
    "    for l in bucket_list:\n",
    "        if prefix in l:\n",
    "            suffix = l.replace('nopal-' + str(q) + '-','')\n",
    "            break\n",
    "    if suffix:\n",
    "        buckets[q] = suffix    \n",
    "\n",
    "for q in QUERY_SIZES:\n",
    "    if q not in buckets:\n",
    "        suffix = str(uuid.uuid4())\n",
    "        inbucket = 'nopal-' + str(q) + '-' + suffix\n",
    "        outbucket = 'nopal-results-' + str(q) + '-' + suffix\n",
    "        buckets[q] = suffix \n",
    "        \n",
    "        !aws s3 mb s3://{inbucket} --region {REGION}          \n",
    "        !aws s3 mb s3://{outbucket} --region {REGION}\n",
    "        if TAGFILE_S3:\n",
    "            !aws s3api put-bucket-tagging --bucket {inbucket} --tagging file://{TAGFILE_S3} \n",
    "            !aws s3api put-bucket-tagging --bucket {outbucket} --tagging file://{TAGFILE_S3} \n",
    "        !aws s3 cp {q}/fasta/ s3://{inbucket}/ --recursive\n",
    "        \n",
    "for q in buckets:\n",
    "    print('Query size: {0}\\n\\tin-bucket: nopal-{0}-{1}\\n\\tout-bucket: nopal-results{0}-{1}'.format(q, buckets[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a AWS Batch unmanaged Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Internet gateway.\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/reference/ec2/create-internet-gateway.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Internet Gateway: igw-0941f7aeb58334343\n"
     ]
    }
   ],
   "source": [
    "igw = !aws ec2 describe-internet-gateways --filters Name=tag:Project,Values={PROJECT}\n",
    "igw = json.loads(''.join(igw))   \n",
    "if 'InternetGateways' in igw and len(igw['InternetGateways']) > 0:\n",
    "    igw = igw['InternetGateways'][0]\n",
    "    print('Using Internet Gateway: {}'.format(igw['InternetGatewayId']))\n",
    "else:\n",
    "    igw = !aws ec2 create-internet-gateway\n",
    "    igw = json.loads(''.join(igw))\n",
    "    if 'InternetGateway' in igw and 'InternetGatewayId' in igw['InternetGateway']:\n",
    "        igw = igw['InternetGateway']\n",
    "        print('Created Internet Gateway: {}'.format(igw['InternetGatewayId']))\n",
    "        if TAGFILE:\n",
    "            igw_id = igw['InternetGatewayId']\n",
    "            !aws ec2 create-tags --resources {igw_id} --cli-input-json file://{TAGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Amazon Virtual Private Cloud  (VPC) and all its componets\n",
    "\n",
    "* VPC: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-vpc.html\n",
    "* ACL: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-network-acl.html\n",
    "* Route Table: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-route-table.html\n",
    "* Subnet: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-subnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VPC: vpc-0811acb4a1a497102\n",
      "Subnet subnet-076bdd7bddb7c51cc attached to VPC vpc-0811acb4a1a497102\n"
     ]
    }
   ],
   "source": [
    "vpc = !aws ec2 describe-vpcs --filters Name=tag:Project,Values={PROJECT}\n",
    "vpc = json.loads(''.join(vpc))\n",
    "if 'Vpcs' in vpc and len(vpc['Vpcs']) > 0:\n",
    "    vpc = vpc['Vpcs'][0]\n",
    "    vpc_id = vpc['VpcId']\n",
    "    print('Using VPC: {}'.format(vpc['VpcId']))\n",
    "    subnet = !aws ec2 describe-subnets --filters \"Name=vpc-id,Values={vpc_id}\"\n",
    "    subnet = json.loads(''.join(subnet))\n",
    "    if 'Subnets' in subnet:\n",
    "        subnet = subnet['Subnets'][0]   \n",
    "        subnet_id = subnet['SubnetId']\n",
    "\n",
    "        print('Subnet {} attached to VPC {}'.format(subnet_id, vpc_id))    \n",
    "else:\n",
    "    print('No VPC, creating it ..... ')\n",
    "    vpc = !aws ec2 create-vpc --cidr-block 10.0.0.0/16 --amazon-provided-ipv6-cidr-block \n",
    "    vpc = json.loads(''.join(vpc))\n",
    "    if 'Vpc' in vpc:\n",
    "        vpc = vpc['Vpc']\n",
    "        vpc_id = vpc['VpcId']\n",
    "        print('Created VPC: {}'.format(vpc_id))\n",
    "        # adding Tags if file exists\n",
    "        if TAGFILE:        \n",
    "            !aws ec2 create-tags --resources {vpc_id} --cli-input-json file://{TAGFILE}\n",
    "\n",
    "        # Attaching igw\n",
    "        igw_id = igw['InternetGatewayId']\n",
    "        print('Attaching IGW {} to the VPC: {}'.format(igw_id, vpc_id))\n",
    "        !aws ec2 attach-internet-gateway --internet-gateway-id {igw_id} --vpc-id {vpc_id}\n",
    "\n",
    "        # Retrieving created ACL\n",
    "        acl = !aws ec2 describe-network-acls --filters Name=vpc-id,Values={vpc_id}\n",
    "        acl = json.loads(''.join(acl))\n",
    "        if 'NetworkAcls' in acl and len(acl['NetworkAcls']) == 1:\n",
    "            acl = acl['NetworkAcls'][0]\n",
    "            # adding Tags if file exists\n",
    "            if TAGFILE:                \n",
    "                acl_id = acl['NetworkAclId']\n",
    "                print('Tagging ACL {}'.format(acl_id))\n",
    "                !aws ec2 create-tags --resources {acl_id} --cli-input-json file://{TAGFILE}\n",
    "\n",
    "        # Retrieving created routes\n",
    "        route = !aws ec2 describe-route-tables --filters Name=vpc-id,Values={vpc_id}\n",
    "        route = json.loads(''.join(route))\n",
    "        if 'RouteTables' in route and len(route['RouteTables']) == 1:\n",
    "            route = route['RouteTables'][0]\n",
    "            route_id = route['RouteTableId']\n",
    "                \n",
    "            route_igw = !aws ec2 create-route --route-table-id {route_id} --destination-cidr-block 0.0.0.0/0 --gateway-id {igw_id}\n",
    "            route_igw = json.loads(''.join(route_igw))\n",
    "            if 'Return' in route_igw and route_igw['Return']:\n",
    "                print('IGW {} attached to route {}'.format(igw_id, route_id))\n",
    "            \n",
    "            # adding Tags if file exists\n",
    "            if TAGFILE:\n",
    "                print('Tagging Route {}'.format(route_id))\n",
    "                !aws ec2 create-tags --resources {route_id} --cli-input-json file://{TAGFILE}\n",
    "\n",
    "        # Creating Subnets\n",
    "        subnet = !aws ec2 create-subnet --vpc-id {vpc_id} --cidr-block 10.0.0.0/16 --availability-zone {ZONE}\n",
    "        subnet = json.loads(''.join(subnet))\n",
    "        if 'Subnet' in subnet:\n",
    "            subnet = subnet['Subnet']   \n",
    "            subnet_id = subnet['SubnetId']\n",
    "            \n",
    "            print('Subnet {} attached to VPC {}'.format(subnet_id, vpc_id))\n",
    "            !aws ec2 modify-subnet-attribute --subnet-id {subnet_id} --map-public-ip-on-launch\n",
    "            print('Public IPs enable on subnet {}'.format(subnet_id))\n",
    "            \n",
    "            # adding Tags if file exists\n",
    "            if TAGFILE:\n",
    "                print('Tagging subnet {}'.format(subnet_id))\n",
    "                !aws ec2 create-tags --resources {subnet_id} --cli-input-json file://{TAGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating EC2 components\n",
    "\n",
    " * Security Group: https://docs.aws.amazon.com/cli/latest/reference/ec2/create-security-group.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sg sg-05c7b9ccd93afc9c1\n"
     ]
    }
   ],
   "source": [
    "sg = !aws ec2 describe-security-groups --filters Name=tag:Project,Values={PROJECT}\n",
    "sg = json.loads(''.join(sg))\n",
    "if 'SecurityGroups' in sg and len(sg['SecurityGroups']) >= 1:\n",
    "    sg = sg['SecurityGroups'][0]\n",
    "    sg_id = sg['GroupId']\n",
    "    print('Using sg {}'.format(sg_id))\n",
    "else:\n",
    "    sg_name = PROJECT\n",
    "    sg_descr = 'Security Group for project: ' + PROJECT\n",
    "    sg = !aws ec2 create-security-group --group-name {sg_name}  --description \"{sg_descr}\" --vpc-id {vpc_id}\n",
    "    sg = json.loads(''.join(sg))\n",
    "    if 'GroupId' in sg:\n",
    "        sg_id = sg['GroupId']\n",
    "        print('SG {} created'.format(sg_id))\n",
    "        \n",
    "        print('Adding SSH inbound to the sg {}'.format(sg_id))\n",
    "        !aws ec2 authorize-security-group-ingress --group-id {sg_id} --protocol tcp --port 22 --cidr 0.0.0.0/0\n",
    "        \n",
    "        # adding Tags if file exists\n",
    "        if TAGFILE:\n",
    "            print('Tagging sg {}'.format(sg_id))\n",
    "            !aws ec2 create-tags --resources {sg_id} --cli-input-json file://{TAGFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching 20 instances for the cluster\n",
    "\n",
    "Using default default Amazon Linux 2 AMI (HVM), SSD Volume Type (ami-0fc61db8544a617ed).\n",
    "In the user data script the following services are installed and configure:\n",
    "\n",
    " * Amazon ECs client\n",
    " * The /dev/nvme1n1 and /dev/nvme2n1 are conifgured with mdadm RAID0, formated (XFS) and mounted in /data\n",
    " * User ec2-user is added to the docker group\n",
    " * The ecs services is configured and started\n",
    " \n",
    "For accessing to the instance using SSH require a SSH key created in the EC2 console (see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching instances\n",
    "\n",
    "To avoid submitting hundreds of instances to an AWS account, we created this cell that will submit a maximum of *TOTAL_RUNNING* instances at execution. Modify this value accordingly to your account and limitation.\n",
    "\n",
    "Run this cell as much as possible after each batch of instances ends.\n",
    "\n",
    "#### Troubleshuting\n",
    "\n",
    " 1. Insufficient Instance Capacity error\n",
    "\n",
    "AWS region is full. Wait for a few hours and re-execute the cell.\n",
    "\n",
    "> ERROR: ['', 'An error occurred (InsufficientInstanceCapacity) when calling the RunInstances operation (reached max retries: 4): We currently do not have sufficient m5dn.16xlarge capacity in the Availability Zone you requested (us-east-1c). Our system will be working on provisioning additional capacity. You can currently get m5dn.16xlarge capacity by not specifying an Availability Zone in your request or choosing us-east-1a, us-east-1b, us-east-1d, us-east-1f.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20 m5dn.8xlarge 10000_10 i-0d18053d3ad3774e7 submitted\n",
      "1/20 m5dn.8xlarge 10000_11 i-049b569cf33428d8e submitted\n",
      "2/20 m5dn.8xlarge 10000_12 i-087398206d3f1f00d submitted\n",
      "3/20 m5dn.8xlarge 10000_13 i-032800a57ef06054b submitted\n",
      "4/20 m5dn.8xlarge 10000_14 i-0f044d86f79cbdf42 submitted\n",
      "5/20 m5dn.8xlarge 10000_15 i-0a2a90f125e08d664 submitted\n",
      "6/20 m5dn.8xlarge 10000_16 i-0bb426bd9d43528ef submitted\n",
      "7/20 m5dn.8xlarge 10000_17 i-0b140ef962529c142 submitted\n",
      "8/20 m5dn.8xlarge 10000_18 i-097bd26d274fa40ac submitted\n",
      "9/20 m5dn.8xlarge 10000_19 i-00c59f1dee6e315fa submitted\n",
      "ERROR: ['', 'An error occurred (InsufficientInstanceCapacity) when calling the RunInstances operation (reached max retries: 4): We currently do not have sufficient m5dn.8xlarge capacity in the Availability Zone you requested (us-east-1c). Our system will be working on provisioning additional capacity. You can currently get m5dn.8xlarge capacity by not specifying an Availability Zone in your request or choosing us-east-1a, us-east-1b, us-east-1d, us-east-1f.']\n"
     ]
    }
   ],
   "source": [
    "# Amazon image\n",
    "IMAGE = \"ami-0fc61db8544a617ed\"\n",
    "\n",
    "count = 0\n",
    "TOTAL_RUNNING = 20\n",
    "\n",
    "for q in QUERY_SIZES:\n",
    "    if q not in instances:\n",
    "        instances[q] = {}\n",
    "    for m in MACHINE_TYPES:        \n",
    "        for c in CPUs:\n",
    "            INSTANCE_TYPE = '{}.{}'.format(m,c['name'])\n",
    "            if INSTANCE_TYPE not in instances[q]:\n",
    "                instances[q][INSTANCE_TYPE] = {}\n",
    "            \n",
    "            CPU = c['CPUs']\n",
    "            inbucket = 'nopal-' + str(q) + '-' + buckets[q]\n",
    "            outbucket = 'nopal-results-' + str(q) + '-' + buckets[q]\n",
    "\n",
    "            DOCKER_CMD = 'time ( docker run -v /data:/data gcr.io/cbb-research-dl/transannot /data/aws-pipeline.sh'\n",
    "            DOCKER_ARG = '-k {0} -p {1} -m {2} -c {3} '.format(access_key[0], secret_access_key[0], INSTANCE_TYPE, CPU)\n",
    "            DOCKER_ARG += '-i {0} -o {1} '.format(inbucket, outbucket)\n",
    "            for i in range(1,21):\n",
    "                SAMPLE = '{}_{}'.format(q, i)\n",
    "                if SAMPLE not in instances[q][INSTANCE_TYPE]:                    \n",
    "                    user_data = 'cat <<EOF >workload.sh\\n{0} {1} -s {2} ) > /data/pipeline.log 2>&1\\n'.format(DOCKER_CMD, DOCKER_ARG, SAMPLE, INSTANCE_TYPE)\n",
    "                    user_data += 'aws configure set aws_access_key_id {}\\n'.format(access_key[0])\n",
    "                    user_data += 'aws configure set aws_secret_access_key {}\\n'.format(secret_access_key[0])\n",
    "                    user_data += 'aws s3 cp /data/pipeline.log s3://{2}/{0}_{1}/\\n'.format(SAMPLE, INSTANCE_TYPE, outbucket)\n",
    "                    user_data += 'shutdown 0\\nEOF\\n'\n",
    "                    user_data += 'chmod a+x workload.sh\\n'\n",
    "                    user_data += './workload.sh &\\n'\n",
    "                    user_datafile = os.path.join(CONFIG, 'aws', 'user-data.txt')\n",
    "                    user_datafile_ecs = os.path.join(CONFIG, 'aws', 'user-data-ecs.txt')\n",
    "                    with open(user_datafile_ecs) as fin:\n",
    "                        with open(user_datafile, 'w') as fout:\n",
    "                            fout.write(fin.read() + '\\n')\n",
    "                            fout.write(user_data + '\\n')\n",
    "\n",
    "                    cmd = 'aws ec2 run-instances --image-id {} --count 1 --instance-type {} '.format(IMAGE, INSTANCE_TYPE)\n",
    "                    cmd += '--placement \"AvailabilityZone={}\" '.format(ZONE)\n",
    "                    cmd += '--iam-instance-profile Name=ecsInstanceRole '\n",
    "                    cmd += '--security-group-ids {} --subnet-id {} '.format(sg_id, subnet_id)\n",
    "                    cmd += '--user-data file://{} '.format(user_datafile)\n",
    "                    cmd += '--instance-initiated-shutdown-behavior terminate '\n",
    "                    cmd += '--tag-specifications '\n",
    "\n",
    "                    #Using defined tags in the instances\n",
    "                    if TAGDICT:\n",
    "                        tags = ''\n",
    "                        for t in TAGDICT['Tags']:\n",
    "                            if tags:\n",
    "                                tags += ','\n",
    "                            tags += '{' + 'Key={0},Value={1}'.format(t['Key'], t['Value']) + '}'\n",
    "                        cmd += '\\'ResourceType=instance,Tags=[{}]\\' '.format(tags)\n",
    "                        cmd += '\\'ResourceType=volume,Tags=[{}]\\''.format(tags)\n",
    "                    \n",
    "                    output = !{cmd}\n",
    "                    try:\n",
    "                        instance = json.loads(''.join(output))\n",
    "                        instances[q][INSTANCE_TYPE][SAMPLE] = instance\n",
    "                        print('{}/{} {} {} {} submitted'.format(count,TOTAL_RUNNING,INSTANCE_TYPE, SAMPLE, instance['Instances'][0]['InstanceId']))\n",
    "                        count += 1                        \n",
    "                    except:\n",
    "                        print('ERROR: {}'.format(output))\n",
    "                        count = TOTAL_RUNNING\n",
    "                    if count >= TOTAL_RUNNING:\n",
    "                        break\n",
    "            if count >= TOTAL_RUNNING:\n",
    "                break\n",
    "        if count >= TOTAL_RUNNING:\n",
    "            break\n",
    "    if count >= TOTAL_RUNNING:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting total times from AWS Cloudtrail\n",
    "\n",
    "Run this cell after all instances are terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in QUERY_SIZES:\n",
    "    if q in instances:        \n",
    "        for m in MACHINE_TYPES:        \n",
    "            for c in CPUs:\n",
    "                INSTANCE_TYPE = '{}.{}'.format(m,c['name'])\n",
    "                if INSTANCE_TYPE in instances[q]:\n",
    "                    instace_dir = os.path.join(result_dir, str(q), m, str(c['CPUs']))\n",
    "                    if not os.path.exists(instace_dir):\n",
    "                        os.makedirs(instace_dir)\n",
    "                    for i in range(1,21):\n",
    "                        SAMPLE = '{}_{}'.format(q, i)\n",
    "                        if SAMPLE in instances[q][INSTANCE_TYPE]:\n",
    "                            instance_id = instances[q][INSTANCE_TYPE][SAMPLE]['Instances'][0]['InstanceId']\n",
    "                            output = !aws cloudtrail lookup-events --lookup-attributes AttributeKey=ResourceName,AttributeValue={instance_id} \n",
    "                            try:\n",
    "                                trail = json.loads(''.join(output))\n",
    "                                start = None\n",
    "                                end = None\n",
    "                                for e in trail['Events']:\n",
    "                                    if 'EventName' in e and e['EventName'] == 'RunInstances':\n",
    "                                        start = float(e['EventTime'])\n",
    "                                    if 'EventName' in e and e['EventName'] == 'TerminateInstances':\n",
    "                                        end = float(e['EventTime'])    \n",
    "                                if start and end:\n",
    "                                    print('{} {} {}'.format(instance_id, start, end))\n",
    "                                    file_name = os.path.join(instace_dir, '{}.json.gz'.format(i))\n",
    "                                    with gzip.GzipFile(file_name, 'w') as fout:   # 4. gzip\n",
    "                                        fout.write(json.dumps(trail, indent=2).encode('utf-8'))  \n",
    "                            except:\n",
    "                                print('ERROR: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
